import { Ollama } from "@llamaindex/ollama";
import { Settings } from "llamaindex";
import readline from "readline";

// Configura el modelo Ollama
// Aseg√∫rate de tener el modelo descargado y corriendo en tu m√°quina
// Baja ollama  de https://ollama.com/ 
// corre el siguiente comando en la terminal: `ollama run gemma3:1b`
const ollamaLLM = new Ollama({
  model: "gemma3:1b", // Cambia el modelo si lo deseas por ejemplo : "mistral:7b", "llama2:7b", etc.
  temperature: 0.75,
});

// Asigna Ollama como LLM y modelo de embeddings
Settings.llm = ollamaLLM;
Settings.embedModel = ollamaLLM;

// Funci√≥n principal del programa
async function main() {

  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });

  const memoria = [
    {
      role: "system",
      content: `
        Eres un asistente vocacional que ayuda a los usuarios a elegir una carrera.
        Vas a hacer al menos 3 preguntas relevantes sobre sus gustos, intereses y preferencias personales durante la conversaci√≥n.
        Sugerir 2 o m√°s carreras, trayectorias educativas o laborales posibles seg√∫n los datos del usuario obtenidos en las respuestas.
        Manten un tono amable, claro, emp√°tica y accesible para todo tipo de usuario en todo momento.
      `
    }
  ];

  console.log("ü§ñ Bot con IA (Ollama) iniciado.");
  console.log("Escrib√≠ tu pregunta o pon√© 'salir' para terminar:");

  // Escuchamos cada vez que el usuario escribe algo
  rl.on("line", async (input) => {
    if (input.toLowerCase() === "salir") {
      rl.close(); // Cerramos el programa si escribi√≥ "salir"
      return;
    }

    memoria.push({
      role: "user",
      content: input, // Lo que escribi√≥ el usuario
    });

    try {
      // Enviamos la pregunta al modelo de IA usando Ollama
      const res = await ollamaLLM.chat({
        messages: memoria,
      });

      // Registramos la respuesta en la memoria
      memoria.push(res?.message);

      // Obtenemos el texto de la respuesta
      const respuesta = res?.message?.content || res?.message || "";

      // Mostramos la respuesta en consola
      console.log("ü§ñ IA:", respuesta.trim());
    } catch (err) {
      // Si hay un error lo mostramos
      console.error("‚ö†Ô∏è Error al llamar al modelo:", err);
    }

    console.log("\nPregunt√° otra cosa o escrib√≠ 'salir':");
  });
}

// Ejecutamos la funci√≥n principal
main();
